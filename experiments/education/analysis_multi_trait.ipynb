{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Trait Education Experiment — Deep Analysis\n",
    "\n",
    "Analyzing how 7 persona steering vectors affect:\n",
    "- **Experiment A**: Student answer quality (steered students, judged by unsteered LLM + OpenAI)\n",
    "- **Experiment B**: Judge grading bias (steered judges scoring real essays with ground truth)\n",
    "\n",
    "Traits: evil, apathetic, hallucinating, humorous, impolite, optimistic, sycophantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "RESULTS_DIR = Path('results/multi_trait_20260206_104247')\n",
    "\n",
    "TRAIT_OPPOSITES = {\n",
    "    'evil': 'good', 'apathetic': 'empathetic', 'hallucinating': 'factual',\n",
    "    'humorous': 'serious', 'impolite': 'polite', 'optimistic': 'pessimistic',\n",
    "    'sycophantic': 'candid',\n",
    "}\n",
    "ALL_TRAITS = list(TRAIT_OPPOSITES.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "def load_jsonl(path):\n",
    "    records = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                records.append(json.loads(line))\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Exp A\n",
    "answers_a = load_jsonl(RESULTS_DIR / 'experiment_a_student/generated_answers.jsonl')\n",
    "scores_a = load_jsonl(RESULTS_DIR / 'experiment_a_student/scoring_results.jsonl')\n",
    "\n",
    "# Exp B\n",
    "scores_b = load_jsonl(RESULTS_DIR / 'experiment_b_judge/scoring_results.jsonl')\n",
    "essays = load_jsonl(RESULTS_DIR / 'shared/sampled_essays.jsonl')\n",
    "\n",
    "# Essay set info\n",
    "with open(RESULTS_DIR / 'shared/essay_sets.json') as f:\n",
    "    essay_sets_info = json.load(f)\n",
    "\n",
    "# Parse trait/direction from student_type and judge_type\n",
    "def parse_type(t):\n",
    "    if t in ('unsteered', 'openai', 'real_student'):\n",
    "        return t, 'none'\n",
    "    parts = t.rsplit('_', 1)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "for df in [scores_a, answers_a]:\n",
    "    df[['trait', 'direction']] = df['student_type'].apply(lambda x: pd.Series(parse_type(x)))\n",
    "\n",
    "scores_b[['judge_trait', 'judge_direction']] = scores_b['judge_type'].apply(lambda x: pd.Series(parse_type(x)))\n",
    "\n",
    "# Add answer length to answers\n",
    "answers_a['answer_len'] = answers_a['answer'].str.len()\n",
    "\n",
    "print(f'Exp A: {len(answers_a)} answers, {len(scores_a)} scores')\n",
    "print(f'Exp B: {len(scores_b)} scores, {len(essays)} real essays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part A: Student Trait Analysis\n",
    "## A1. Overall Effect Sizes — Which traits matter most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute effect sizes: mean score difference vs unsteered baseline\n",
    "baseline = scores_a[scores_a['student_type'] == 'unsteered'].groupby('judge_type')['normalized_score'].mean()\n",
    "\n",
    "effects = []\n",
    "for st in scores_a['student_type'].unique():\n",
    "    if st == 'unsteered':\n",
    "        continue\n",
    "    trait, direction = parse_type(st)\n",
    "    for jt in ['unsteered', 'openai']:\n",
    "        mean_score = scores_a[(scores_a['student_type'] == st) & (scores_a['judge_type'] == jt)]['normalized_score'].mean()\n",
    "        effect = mean_score - baseline[jt]\n",
    "        effects.append({\n",
    "            'trait': trait, 'direction': direction, 'student_type': st,\n",
    "            'judge_type': jt, 'mean_score': round(mean_score, 4),\n",
    "            'baseline': round(baseline[jt], 4), 'effect': round(effect, 4),\n",
    "        })\n",
    "\n",
    "effects_df = pd.DataFrame(effects)\n",
    "\n",
    "# Rank by absolute effect (averaged across both judges)\n",
    "avg_effect = effects_df.groupby(['trait', 'direction'])['effect'].mean().reset_index()\n",
    "avg_effect['abs_effect'] = avg_effect['effect'].abs()\n",
    "avg_effect = avg_effect.sort_values('abs_effect', ascending=False)\n",
    "\n",
    "print('=== Effect Sizes (avg across judges, sorted by magnitude) ===')\n",
    "print(avg_effect[['trait', 'direction', 'effect', 'abs_effect']].to_string(index=False))\n",
    "print(f'\\nBaseline scores: unsteered judge={baseline[\"unsteered\"]:.4f}, openai={baseline[\"openai\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: effect size bar chart per trait (pos vs neg)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "for idx, jt in enumerate(['unsteered', 'openai']):\n",
    "    ax = axes[idx]\n",
    "    subset = effects_df[effects_df['judge_type'] == jt]\n",
    "    \n",
    "    traits_order = []\n",
    "    pos_effects = []\n",
    "    neg_effects = []\n",
    "    for trait in ALL_TRAITS:\n",
    "        pos = subset[(subset['trait'] == trait) & (subset['direction'] == 'pos')]['effect'].values\n",
    "        neg = subset[(subset['trait'] == trait) & (subset['direction'] == 'neg')]['effect'].values\n",
    "        traits_order.append(trait)\n",
    "        pos_effects.append(pos[0] if len(pos) > 0 else 0)\n",
    "        neg_effects.append(neg[0] if len(neg) > 0 else 0)\n",
    "    \n",
    "    x = np.arange(len(traits_order))\n",
    "    width = 0.35\n",
    "    bars1 = ax.bar(x - width/2, pos_effects, width, label='pos (+trait)', color='#d62728', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, neg_effects, width, label='neg (-trait/opposite)', color='#2ca02c', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Trait')\n",
    "    ax.set_ylabel('Effect on normalized score')\n",
    "    ax.set_title(f'Student Steering Effect ({jt} judge)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(traits_order, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'combined/student_effect_sizes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Score Distributions — Consistency of steered students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distribution stats per student type\n",
    "dist_stats = scores_a.groupby(['student_type', 'judge_type']).agg(\n",
    "    mean=('normalized_score', 'mean'),\n",
    "    std=('normalized_score', 'std'),\n",
    "    median=('normalized_score', 'median'),\n",
    "    q25=('normalized_score', lambda x: x.quantile(0.25)),\n",
    "    q75=('normalized_score', lambda x: x.quantile(0.75)),\n",
    "    min=('normalized_score', 'min'),\n",
    "    max=('normalized_score', 'max'),\n",
    "    count=('normalized_score', 'count'),\n",
    ").round(4)\n",
    "\n",
    "print('=== Score Distribution Stats (OpenAI judge) ===')\n",
    "openai_dist = dist_stats.xs('openai', level='judge_type').sort_values('mean', ascending=False)\n",
    "print(openai_dist.to_string())\n",
    "print()\n",
    "print('=== Score Distribution Stats (Unsteered judge) ===')\n",
    "unst_dist = dist_stats.xs('unsteered', level='judge_type').sort_values('mean', ascending=False)\n",
    "print(unst_dist.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots: score distributions per student type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7), sharey=True)\n",
    "\n",
    "for idx, jt in enumerate(['openai', 'unsteered']):\n",
    "    ax = axes[idx]\n",
    "    subset = scores_a[scores_a['judge_type'] == jt]\n",
    "    \n",
    "    # Order by mean score\n",
    "    order = subset.groupby('student_type')['normalized_score'].mean().sort_values(ascending=False).index\n",
    "    \n",
    "    data = [subset[subset['student_type'] == st]['normalized_score'].values for st in order]\n",
    "    bp = ax.boxplot(data, labels=order, vert=True, patch_artist=True)\n",
    "    \n",
    "    # Color by direction\n",
    "    for i, st in enumerate(order):\n",
    "        _, d = parse_type(st)\n",
    "        color = '#d62728' if d == 'pos' else '#2ca02c' if d == 'neg' else '#1f77b4'\n",
    "        bp['boxes'][i].set_facecolor(color)\n",
    "        bp['boxes'][i].set_alpha(0.6)\n",
    "    \n",
    "    ax.set_title(f'Score Distributions ({jt} judge)')\n",
    "    ax.set_ylabel('Normalized Score')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'combined/student_score_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Per-Set Analysis — Which question types are most affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect per set: how does each trait affect scores on different question types?\n",
    "set_topics = {int(k): v['topic'] for k, v in essay_sets_info.items()}\n",
    "\n",
    "baseline_per_set = scores_a[scores_a['student_type'] == 'unsteered'].groupby(\n",
    "    ['set_id', 'judge_type']\n",
    ")['normalized_score'].mean()\n",
    "\n",
    "set_effects = []\n",
    "for st in scores_a['student_type'].unique():\n",
    "    if st == 'unsteered':\n",
    "        continue\n",
    "    trait, direction = parse_type(st)\n",
    "    for sid in scores_a['set_id'].unique():\n",
    "        for jt in ['openai']:\n",
    "            mean = scores_a[(scores_a['student_type'] == st) & \n",
    "                           (scores_a['set_id'] == sid) &\n",
    "                           (scores_a['judge_type'] == jt)]['normalized_score'].mean()\n",
    "            bl = baseline_per_set.get((sid, jt), 0)\n",
    "            set_effects.append({\n",
    "                'trait': trait, 'direction': direction, 'student_type': st,\n",
    "                'set_id': sid, 'topic': set_topics.get(sid, '?'),\n",
    "                'mean_score': round(mean, 4), 'baseline': round(bl, 4),\n",
    "                'effect': round(mean - bl, 4),\n",
    "            })\n",
    "\n",
    "set_effects_df = pd.DataFrame(set_effects)\n",
    "\n",
    "# Heatmap: trait_pos effect per set (OpenAI judge)\n",
    "pos_effects = set_effects_df[set_effects_df['direction'] == 'pos'].pivot_table(\n",
    "    values='effect', index='trait', columns='set_id', aggfunc='mean'\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "for idx, (direction, title) in enumerate([('pos', 'Positive Steering (+trait)'), ('neg', 'Negative Steering (-trait/opposite)')]):\n",
    "    ax = axes[idx]\n",
    "    pivot = set_effects_df[set_effects_df['direction'] == direction].pivot_table(\n",
    "        values='effect', index='trait', columns='set_id', aggfunc='mean'\n",
    "    )\n",
    "    im = ax.imshow(pivot.values, cmap='RdYlGn', aspect='auto', vmin=-0.6, vmax=0.3)\n",
    "    ax.set_xticks(range(len(pivot.columns)))\n",
    "    ax.set_xticklabels(pivot.columns)\n",
    "    ax.set_yticks(range(len(pivot.index)))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "    ax.set_xlabel('Essay Set')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Annotate cells\n",
    "    for i in range(len(pivot.index)):\n",
    "        for j in range(len(pivot.columns)):\n",
    "            val = pivot.values[i, j]\n",
    "            ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=8,\n",
    "                   color='white' if abs(val) > 0.3 else 'black')\n",
    "\n",
    "plt.colorbar(im, ax=axes, shrink=0.8, label='Effect on normalized score')\n",
    "plt.suptitle('Student Steering Effect by Essay Set (OpenAI judge)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'combined/student_effect_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which sets are most/least affected by each trait?\n",
    "print('=== Most Affected Sets Per Trait (pos direction, OpenAI judge) ===')\n",
    "for trait in ALL_TRAITS:\n",
    "    sub = set_effects_df[(set_effects_df['trait'] == trait) & (set_effects_df['direction'] == 'pos')]\n",
    "    worst = sub.loc[sub['effect'].idxmin()]\n",
    "    best = sub.loc[sub['effect'].idxmax()]\n",
    "    print(f'\\n{trait} (+{trait}):')\n",
    "    print(f'  Worst: Set {worst[\"set_id\"]} ({worst[\"topic\"]}) effect={worst[\"effect\"]:+.3f}')\n",
    "    print(f'  Best:  Set {best[\"set_id\"]} ({best[\"topic\"]}) effect={best[\"effect\"]:+.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4. Answer Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer length by student type\n",
    "len_stats = answers_a.groupby('student_type')['answer_len'].agg(['mean', 'std', 'median']).round(1)\n",
    "len_stats = len_stats.sort_values('mean', ascending=False)\n",
    "print('=== Answer Length by Student Type ===')\n",
    "print(len_stats.to_string())\n",
    "\n",
    "# Merge answer length with scores\n",
    "scores_with_len = scores_a.merge(\n",
    "    answers_a[['set_id', 'sample_id', 'student_type', 'answer_len']],\n",
    "    on=['set_id', 'sample_id', 'student_type'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Correlation between length and score\n",
    "print('\\n=== Length-Score Correlation (by student type, OpenAI judge) ===')\n",
    "openai_scores = scores_with_len[scores_with_len['judge_type'] == 'openai']\n",
    "for st in sorted(openai_scores['student_type'].unique()):\n",
    "    sub = openai_scores[openai_scores['student_type'] == st]\n",
    "    if len(sub) > 2 and sub['answer_len'].std() > 0:\n",
    "        corr = sub['answer_len'].corr(sub['normalized_score'])\n",
    "        print(f'  {st:25s} r={corr:+.3f}  (mean_len={sub[\"answer_len\"].mean():.0f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter: answer length vs score for selected traits\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "student_types_to_plot = ['unsteered'] + [f'{t}_pos' for t in ALL_TRAITS]\n",
    "openai_with_len = scores_with_len[scores_with_len['judge_type'] == 'openai']\n",
    "\n",
    "for idx, st in enumerate(student_types_to_plot[:8]):\n",
    "    ax = axes[idx]\n",
    "    sub = openai_with_len[openai_with_len['student_type'] == st]\n",
    "    ax.scatter(sub['answer_len'], sub['normalized_score'], alpha=0.4, s=20)\n",
    "    ax.set_title(st, fontsize=10)\n",
    "    ax.set_xlabel('Answer length (chars)')\n",
    "    ax.set_ylabel('Normalized score')\n",
    "    if len(sub) > 2 and sub['answer_len'].std() > 0:\n",
    "        corr = sub['answer_len'].corr(sub['normalized_score'])\n",
    "        ax.text(0.05, 0.95, f'r={corr:.2f}', transform=ax.transAxes, va='top', fontsize=9)\n",
    "\n",
    "plt.suptitle('Answer Length vs Score (OpenAI judge)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'combined/length_vs_score.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part B: Judge Trait Analysis\n",
    "## B1. Judge Performance vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall judge performance metrics\n",
    "judge_perf = []\n",
    "for jt in scores_b['judge_type'].unique():\n",
    "    sub = scores_b[scores_b['judge_type'] == jt]\n",
    "    pred = sub['raw_score'].values\n",
    "    gt = sub['ground_truth_score'].values\n",
    "    \n",
    "    bias = pred - gt\n",
    "    judge_perf.append({\n",
    "        'judge_type': jt,\n",
    "        'mean_bias': round(np.mean(bias), 3),\n",
    "        'std_bias': round(np.std(bias), 3),\n",
    "        'mae': round(np.mean(np.abs(bias)), 3),\n",
    "        'rmse': round(np.sqrt(np.mean(bias**2)), 3),\n",
    "        'mean_pred': round(np.mean(pred), 3),\n",
    "        'mean_gt': round(np.mean(gt), 3),\n",
    "        'pred_std': round(np.std(pred), 3),\n",
    "        'gt_std': round(np.std(gt), 3),\n",
    "        'corr': round(np.corrcoef(pred, gt)[0,1], 3) if np.std(pred) > 0 else 0,\n",
    "        'count': len(sub),\n",
    "    })\n",
    "\n",
    "judge_perf_df = pd.DataFrame(judge_perf).sort_values('mae')\n",
    "print('=== Judge Performance vs Ground Truth (sorted by MAE) ===')\n",
    "print(judge_perf_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QWK from the experiment results\n",
    "qwk_df = pd.read_csv(RESULTS_DIR / 'experiment_b_judge/qwk_scores.csv')\n",
    "\n",
    "# Merge with parsed trait info\n",
    "qwk_df[['trait', 'direction']] = qwk_df['judge_type'].apply(lambda x: pd.Series(parse_type(x)))\n",
    "\n",
    "print('=== QWK Scores (sorted by mean_qwk) ===')\n",
    "qwk_display = qwk_df[['judge_type', 'trait', 'direction', 'mean_qwk', 'std_qwk', 'mean_bias', 'mae']].sort_values('mean_qwk', ascending=False)\n",
    "print(qwk_display.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: judge bias and QWK\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart: mean bias per judge\n",
    "ax = axes[0]\n",
    "perf_sorted = judge_perf_df.sort_values('mean_bias')\n",
    "colors = []\n",
    "for jt in perf_sorted['judge_type']:\n",
    "    _, d = parse_type(jt)\n",
    "    colors.append('#d62728' if d == 'pos' else '#2ca02c' if d == 'neg' else '#1f77b4' if jt == 'unsteered' else '#ff7f0e')\n",
    "ax.barh(range(len(perf_sorted)), perf_sorted['mean_bias'], color=colors, alpha=0.8)\n",
    "ax.set_yticks(range(len(perf_sorted)))\n",
    "ax.set_yticklabels(perf_sorted['judge_type'], fontsize=9)\n",
    "ax.set_xlabel('Mean Bias (predicted - ground truth)')\n",
    "ax.set_title('Judge Bias')\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Bar chart: mean QWK per judge\n",
    "ax = axes[1]\n",
    "qwk_sorted = qwk_df.sort_values('mean_qwk', ascending=True)\n",
    "colors = []\n",
    "for jt in qwk_sorted['judge_type']:\n",
    "    _, d = parse_type(jt)\n",
    "    colors.append('#d62728' if d == 'pos' else '#2ca02c' if d == 'neg' else '#1f77b4' if jt == 'unsteered' else '#ff7f0e')\n",
    "ax.barh(range(len(qwk_sorted)), qwk_sorted['mean_qwk'], color=colors, alpha=0.8)\n",
    "ax.set_yticks(range(len(qwk_sorted)))\n",
    "ax.set_yticklabels(qwk_sorted['judge_type'], fontsize=9)\n",
    "ax.set_xlabel('Mean QWK')\n",
    "ax.set_title('Judge Agreement with Ground Truth (QWK)')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'combined/judge_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Per-Set Judge Quality — Which question types are hardest to judge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-set QWK heatmap\n",
    "qwk_set_cols = [c for c in qwk_df.columns if c.startswith('qwk_set_')]\n",
    "qwk_set_data = qwk_df.set_index('judge_type')[qwk_set_cols]\n",
    "qwk_set_data.columns = [int(c.replace('qwk_set_', '')) for c in qwk_set_data.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "im = ax.imshow(qwk_set_data.values, cmap='RdYlGn', aspect='auto', vmin=-0.5, vmax=1.0)\n",
    "ax.set_xticks(range(len(qwk_set_data.columns)))\n",
    "ax.set_xticklabels(qwk_set_data.columns)\n",
    "ax.set_yticks(range(len(qwk_set_data.index)))\n",
    "ax.set_yticklabels(qwk_set_data.index, fontsize=9)\n",
    "ax.set_xlabel('Essay Set')\n",
    "ax.set_title('QWK by Judge Type and Essay Set')\n",
    "\n",
    "for i in range(len(qwk_set_data.index)):\n",
    "    for j in range(len(qwk_set_data.columns)):\n",
    "        val = qwk_set_data.values[i, j]\n",
    "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=7,\n",
    "               color='white' if val < 0 or val > 0.7 else 'black')\n",
    "\n",
    "plt.colorbar(im, shrink=0.8, label='QWK')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'combined/judge_qwk_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-set bias heatmap\n",
    "bias_set_cols = [c for c in qwk_df.columns if c.startswith('bias_set_')]\n",
    "bias_set_data = qwk_df.set_index('judge_type')[bias_set_cols]\n",
    "bias_set_data.columns = [int(c.replace('bias_set_', '')) for c in bias_set_data.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "im = ax.imshow(bias_set_data.values, cmap='RdBu_r', aspect='auto', vmin=-1.5, vmax=1.5)\n",
    "ax.set_xticks(range(len(bias_set_data.columns)))\n",
    "ax.set_xticklabels(bias_set_data.columns)\n",
    "ax.set_yticks(range(len(bias_set_data.index)))\n",
    "ax.set_yticklabels(bias_set_data.index, fontsize=9)\n",
    "ax.set_xlabel('Essay Set')\n",
    "ax.set_title('Bias by Judge Type and Essay Set (predicted - ground truth)')\n",
    "\n",
    "for i in range(len(bias_set_data.index)):\n",
    "    for j in range(len(bias_set_data.columns)):\n",
    "        val = bias_set_data.values[i, j]\n",
    "        ax.text(j, i, f'{val:.1f}', ha='center', va='center', fontsize=7,\n",
    "               color='white' if abs(val) > 1.0 else 'black')\n",
    "\n",
    "plt.colorbar(im, shrink=0.8, label='Bias')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'combined/judge_bias_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Score Compression — Do steered judges lose score range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score range usage per judge\n",
    "score_range_stats = scores_b.groupby('judge_type').agg(\n",
    "    pred_std=('raw_score', 'std'),\n",
    "    pred_range=('raw_score', lambda x: x.max() - x.min()),\n",
    "    unique_scores=('raw_score', 'nunique'),\n",
    "    mode_score=('raw_score', lambda x: x.mode()[0] if len(x.mode()) > 0 else None),\n",
    "    mode_pct=('raw_score', lambda x: (x == x.mode()[0]).mean() if len(x.mode()) > 0 else 0),\n",
    ").round(3)\n",
    "\n",
    "print('=== Score Range Usage by Judge (compression analysis) ===')\n",
    "print(score_range_stats.sort_values('pred_std', ascending=False).to_string())\n",
    "print('\\nLower std = more compressed scoring. Higher mode_pct = more repetitive.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part C: Cross-Experiment Analysis\n",
    "## C1. Student Effect vs Judge Bias Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each trait: student effect (Exp A) vs judge bias (Exp B)\n",
    "comparison = pd.read_csv(RESULTS_DIR / 'combined/trait_comparison.csv')\n",
    "\n",
    "# Get student pos effect (avg of both judges) and judge pos bias\n",
    "cross_data = []\n",
    "for trait in ALL_TRAITS:\n",
    "    # Student effect from Exp A (average of both judges)\n",
    "    student_rows = comparison[(comparison['trait'] == trait) & (comparison['experiment'] == 'A_student')]\n",
    "    student_pos_effect = student_rows['pos_effect'].mean()\n",
    "    student_neg_effect = student_rows['neg_effect'].mean()\n",
    "    \n",
    "    # Judge bias from Exp B\n",
    "    judge_row = comparison[(comparison['trait'] == trait) & (comparison['experiment'] == 'B_judge')]\n",
    "    if len(judge_row) > 0:\n",
    "        judge_pos_effect = judge_row['pos_effect'].values[0]\n",
    "        judge_neg_effect = judge_row['neg_effect'].values[0]\n",
    "    else:\n",
    "        judge_pos_effect = judge_neg_effect = 0\n",
    "    \n",
    "    cross_data.append({\n",
    "        'trait': trait, 'opposite': TRAIT_OPPOSITES[trait],\n",
    "        'student_pos_effect': round(student_pos_effect, 4),\n",
    "        'student_neg_effect': round(student_neg_effect, 4),\n",
    "        'judge_pos_bias': round(judge_pos_effect, 4),\n",
    "        'judge_neg_bias': round(judge_neg_effect, 4),\n",
    "    })\n",
    "\n",
    "cross_df = pd.DataFrame(cross_data)\n",
    "print('=== Cross-Experiment: Student Effect vs Judge Bias ===')\n",
    "print(cross_df.to_string(index=False))\n",
    "\n",
    "# Scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for idx, direction in enumerate(['pos', 'neg']):\n",
    "    ax = axes[idx]\n",
    "    x = cross_df[f'student_{direction}_effect']\n",
    "    y = cross_df[f'judge_{direction}_bias']\n",
    "    \n",
    "    ax.scatter(x, y, s=100, zorder=5)\n",
    "    for i, trait in enumerate(cross_df['trait']):\n",
    "        ax.annotate(trait, (x.iloc[i], y.iloc[i]), textcoords='offset points',\n",
    "                   xytext=(8, 5), fontsize=10)\n",
    "    \n",
    "    ax.set_xlabel(f'Student Effect ({direction})')\n",
    "    ax.set_ylabel(f'Judge Bias ({direction})')\n",
    "    ax.set_title(f'{direction.upper()} direction')\n",
    "    ax.axhline(y=0, color='gray', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='gray', linewidth=0.5)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    if x.std() > 0 and y.std() > 0:\n",
    "        corr = np.corrcoef(x, y)[0, 1]\n",
    "        ax.text(0.05, 0.95, f'r={corr:.2f}', transform=ax.transAxes, va='top', fontsize=11)\n",
    "\n",
    "plt.suptitle('Does a trait that hurts students also bias judges?', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'combined/cross_experiment_correlation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2. Trait Clustering — Which traits have similar effects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature vectors per trait: [student_pos_effect_per_set..., student_neg_effect_per_set..., judge_bias, qwk]\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "trait_features = []\n",
    "set_ids = sorted(scores_a['set_id'].unique())\n",
    "\n",
    "for trait in ALL_TRAITS:\n",
    "    feats = {'trait': trait}\n",
    "    \n",
    "    # Student effects per set (pos and neg)\n",
    "    for direction in ['pos', 'neg']:\n",
    "        st = f'{trait}_{direction}'\n",
    "        for sid in set_ids:\n",
    "            mean = scores_a[\n",
    "                (scores_a['student_type'] == st) & \n",
    "                (scores_a['set_id'] == sid) & \n",
    "                (scores_a['judge_type'] == 'openai')\n",
    "            ]['normalized_score'].mean()\n",
    "            feats[f's_{direction}_set{sid}'] = mean\n",
    "    \n",
    "    # Judge bias (pos and neg)\n",
    "    for direction in ['pos', 'neg']:\n",
    "        jt = f'{trait}_{direction}'\n",
    "        sub = scores_b[scores_b['judge_type'] == jt]\n",
    "        feats[f'j_{direction}_bias'] = (sub['raw_score'] - sub['ground_truth_score']).mean()\n",
    "        feats[f'j_{direction}_mae'] = (sub['raw_score'] - sub['ground_truth_score']).abs().mean()\n",
    "    \n",
    "    trait_features.append(feats)\n",
    "\n",
    "trait_feat_df = pd.DataFrame(trait_features).set_index('trait')\n",
    "\n",
    "# Dendrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "linkage_matrix = linkage(pdist(trait_feat_df.values, 'euclidean'), method='ward')\n",
    "dendrogram(linkage_matrix, labels=trait_feat_df.index.tolist(), ax=ax, leaf_font_size=12)\n",
    "ax.set_title('Trait Similarity (hierarchical clustering on student + judge effects)')\n",
    "ax.set_ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'combined/trait_clustering.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3. Asymmetry Analysis — Is positive always worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each trait × set: compare pos vs neg effect\n",
    "asymmetry_data = []\n",
    "for trait in ALL_TRAITS:\n",
    "    for sid in set_ids:\n",
    "        for jt in ['openai']:\n",
    "            pos_mean = scores_a[\n",
    "                (scores_a['student_type'] == f'{trait}_pos') &\n",
    "                (scores_a['set_id'] == sid) & (scores_a['judge_type'] == jt)\n",
    "            ]['normalized_score'].mean()\n",
    "            neg_mean = scores_a[\n",
    "                (scores_a['student_type'] == f'{trait}_neg') &\n",
    "                (scores_a['set_id'] == sid) & (scores_a['judge_type'] == jt)\n",
    "            ]['normalized_score'].mean()\n",
    "            bl = baseline_per_set.get((sid, jt), 0)\n",
    "            \n",
    "            asymmetry_data.append({\n",
    "                'trait': trait, 'set_id': sid,\n",
    "                'pos_effect': round(pos_mean - bl, 4),\n",
    "                'neg_effect': round(neg_mean - bl, 4),\n",
    "                'asymmetry': round((pos_mean - bl) - (neg_mean - bl), 4),  # pos - neg\n",
    "                'neg_better_than_pos': neg_mean > pos_mean,\n",
    "            })\n",
    "\n",
    "asym_df = pd.DataFrame(asymmetry_data)\n",
    "\n",
    "# Count reversals: cases where neg direction scores LOWER than pos\n",
    "print('=== Asymmetry: How often is neg (opposite) better than pos? ===')\n",
    "for trait in ALL_TRAITS:\n",
    "    sub = asym_df[asym_df['trait'] == trait]\n",
    "    pct_neg_better = sub['neg_better_than_pos'].mean() * 100\n",
    "    mean_asym = sub['asymmetry'].mean()\n",
    "    print(f'  {trait:20s} neg>pos in {pct_neg_better:.0f}% of sets, mean asymmetry={mean_asym:+.3f}')\n",
    "\n",
    "print('\\n(Positive asymmetry = pos is worse than neg, as expected)')\n",
    "print('(Reversals = cases where opposite-steered student scores LOWER)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part D: Cherry-Picked Cases\n",
    "## D1. Student Cases — Answers that reveal trait patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge scores with answers for cherry picking\n",
    "merged_a = scores_a[scores_a['judge_type'] == 'openai'].merge(\n",
    "    answers_a[['set_id', 'sample_id', 'student_type', 'answer', 'answer_len']],\n",
    "    on=['set_id', 'sample_id', 'student_type'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "def show_case(row, label=''):\n",
    "    print(f'\\n{\"=\" * 80}')\n",
    "    print(f'{label}')\n",
    "    print(f'Set {row[\"set_id\"]} | {row[\"student_type\"]} | Score: {row[\"raw_score\"]}/{row[\"score_range\"][1] if isinstance(row[\"score_range\"], list) else \"?\"} (norm={row[\"normalized_score\"]:.2f})')\n",
    "    print(f'Answer ({row[\"answer_len\"]:.0f} chars):')\n",
    "    print(f'{row[\"answer\"][:500]}')\n",
    "    if row['answer_len'] > 500:\n",
    "        print(f'... [{row[\"answer_len\"]:.0f} total chars]')\n",
    "    print(f'{\"=\" * 80}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most extreme cases: steered student scores much worse than unsteered\n",
    "# Compare each steered answer to the unsteered answer for the same (set_id, sample_id)\n",
    "unsteered_scores = merged_a[merged_a['student_type'] == 'unsteered'][['set_id', 'sample_id', 'normalized_score']].rename(\n",
    "    columns={'normalized_score': 'unsteered_score'}\n",
    ")\n",
    "\n",
    "steered_vs_unsteered = merged_a[merged_a['student_type'] != 'unsteered'].merge(\n",
    "    unsteered_scores, on=['set_id', 'sample_id'], how='left'\n",
    ")\n",
    "steered_vs_unsteered['score_drop'] = steered_vs_unsteered['normalized_score'] - steered_vs_unsteered['unsteered_score']\n",
    "\n",
    "print('=== WORST CASES: Biggest score drops from steering ===')\n",
    "worst = steered_vs_unsteered.nsmallest(5, 'score_drop')\n",
    "for _, row in worst.iterrows():\n",
    "    show_case(row, f'WORST DROP: {row[\"student_type\"]} scored {row[\"score_drop\"]:+.2f} vs unsteered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== BEST CASES: Steering unexpectedly improves score ===')\n",
    "best = steered_vs_unsteered.nlargest(5, 'score_drop')\n",
    "for _, row in best.iterrows():\n",
    "    show_case(row, f'BEST IMPROVEMENT: {row[\"student_type\"]} scored {row[\"score_drop\"]:+.2f} vs unsteered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trait-specific cherry picks: show one example per trait where the trait is clearly visible\n",
    "print('=== TRAIT-REVEALING EXAMPLES (pos direction, lowest scoring) ===')\n",
    "for trait in ALL_TRAITS:\n",
    "    sub = merged_a[(merged_a['student_type'] == f'{trait}_pos')].nsmallest(1, 'normalized_score')\n",
    "    if len(sub) > 0:\n",
    "        row = sub.iloc[0]\n",
    "        show_case(row, f'TRAIT: {trait} (+pos) — lowest scoring example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: same question, unsteered vs humorous_pos (biggest effect trait)\n",
    "print('=== SIDE-BY-SIDE: Unsteered vs humorous_pos (same prompt) ===')\n",
    "# Pick a case where humorous_pos scored much lower\n",
    "humor_drops = steered_vs_unsteered[steered_vs_unsteered['student_type'] == 'humorous_pos'].nsmallest(3, 'score_drop')\n",
    "\n",
    "for _, humor_row in humor_drops.iterrows():\n",
    "    sid, samp = humor_row['set_id'], humor_row['sample_id']\n",
    "    unst_row = merged_a[(merged_a['set_id'] == sid) & (merged_a['sample_id'] == samp) & (merged_a['student_type'] == 'unsteered')]\n",
    "    if len(unst_row) > 0:\n",
    "        print(f'\\n{\"#\" * 80}')\n",
    "        print(f'Set {sid}, Sample {samp}')\n",
    "        print(f'\\n--- UNSTEERED (score={unst_row.iloc[0][\"normalized_score\"]:.2f}) ---')\n",
    "        print(unst_row.iloc[0]['answer'][:400])\n",
    "        print(f'\\n--- HUMOROUS_POS (score={humor_row[\"normalized_score\"]:.2f}) ---')\n",
    "        print(humor_row['answer'][:400])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D2. Judge Cases — How steered judges misjudge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cases where steered judges deviate most from ground truth\n",
    "scores_b['error'] = scores_b['raw_score'] - scores_b['ground_truth_score']\n",
    "scores_b['abs_error'] = scores_b['error'].abs()\n",
    "\n",
    "# Merge with essay text\n",
    "merged_b = scores_b.merge(\n",
    "    essays[['essay_id', 'essay_text', 'avg_score']],\n",
    "    left_on='essay_id', right_on='essay_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "def show_judge_case(row, label=''):\n",
    "    print(f'\\n{\"=\" * 80}')\n",
    "    print(f'{label}')\n",
    "    print(f'Essay {row[\"essay_id\"]} (Set {row[\"set_id\"]}) | Judge: {row[\"judge_type\"]}')\n",
    "    print(f'Predicted: {row[\"raw_score\"]} | Ground Truth: {row[\"ground_truth_score\"]} | Error: {row[\"error\"]:+.1f}')\n",
    "    print(f'Essay text ({len(str(row[\"essay_text\"]))} chars):')\n",
    "    print(str(row['essay_text'])[:400])\n",
    "    if len(str(row['essay_text'])) > 400:\n",
    "        print(f'... [{len(str(row[\"essay_text\"]))} total chars]')\n",
    "    print(f'{\"=\" * 80}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biggest overscoring by steered judges (excluding openai)\n",
    "steered_judges = merged_b[~merged_b['judge_type'].isin(['openai', 'unsteered'])]\n",
    "\n",
    "print('=== BIGGEST OVERSCORING by steered judges ===')\n",
    "overscored = steered_judges.nlargest(5, 'error')\n",
    "for _, row in overscored.iterrows():\n",
    "    show_judge_case(row, f'OVERSCORED by {row[\"error\"]:+.1f}')\n",
    "\n",
    "print('\\n\\n=== BIGGEST UNDERSCORING by steered judges ===')\n",
    "underscored = steered_judges.nsmallest(5, 'error')\n",
    "for _, row in underscored.iterrows():\n",
    "    show_judge_case(row, f'UNDERSCORED by {row[\"error\"]:+.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: same essay, different judges — find max disagreement\n",
    "score_matrix_b = pd.read_csv(RESULTS_DIR / 'experiment_b_judge/score_matrix.csv')\n",
    "\n",
    "# Find essays with biggest spread across judges\n",
    "raw_cols = [c for c in score_matrix_b.columns if c.endswith('_raw')]\n",
    "score_matrix_b['judge_spread'] = score_matrix_b[raw_cols].max(axis=1) - score_matrix_b[raw_cols].min(axis=1)\n",
    "score_matrix_b['judge_std'] = score_matrix_b[raw_cols].std(axis=1)\n",
    "\n",
    "print('=== MOST CONTROVERSIAL ESSAYS (biggest spread across judges) ===')\n",
    "controversial = score_matrix_b.nlargest(5, 'judge_spread')\n",
    "for _, row in controversial.iterrows():\n",
    "    eid = row.get('essay_id', '?')\n",
    "    gt = row.get('ground_truth', '?')\n",
    "    print(f'\\nEssay {eid} (Set {row[\"set_id\"]}) | GT={gt} | Spread={row[\"judge_spread\"]:.0f} | Std={row[\"judge_std\"]:.2f}')\n",
    "    scores = {c.replace('_raw', ''): row[c] for c in raw_cols if pd.notna(row[c])}\n",
    "    scores_sorted = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for judge, score in scores_sorted:\n",
    "        marker = ' <<<' if abs(score - gt) > 1.5 else ''\n",
    "        print(f'  {judge:25s}: {score:.0f}{marker}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-trait: sycophantic judges — do they always score higher? (lenient)\n",
    "# vs evil judges — do they score lower? (harsh)\n",
    "print('=== JUDGE LENIENCY PATTERN ===')\n",
    "print('\\nMean raw score by judge type (vs ground truth mean):')\n",
    "gt_mean = scores_b['ground_truth_score'].mean()\n",
    "print(f'  Ground truth mean: {gt_mean:.2f}\\n')\n",
    "\n",
    "leniency = scores_b.groupby('judge_type').agg(\n",
    "    mean_pred=('raw_score', 'mean'),\n",
    "    mean_error=('error', 'mean'),\n",
    ").round(3).sort_values('mean_pred', ascending=False)\n",
    "\n",
    "for jt, row in leniency.iterrows():\n",
    "    direction = 'LENIENT' if row['mean_error'] > 0.1 else 'HARSH' if row['mean_error'] < -0.1 else 'NEUTRAL'\n",
    "    print(f'  {jt:25s} pred={row[\"mean_pred\"]:.2f}  bias={row[\"mean_error\"]:+.2f}  [{direction}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary: one row per trait with key metrics from both experiments\n",
    "summary = []\n",
    "for trait in ALL_TRAITS:\n",
    "    row = {'trait': trait, 'opposite': TRAIT_OPPOSITES[trait]}\n",
    "    \n",
    "    # Student effects (avg of both judges)\n",
    "    for d in ['pos', 'neg']:\n",
    "        st = f'{trait}_{d}'\n",
    "        mean = scores_a[scores_a['student_type'] == st]['normalized_score'].mean()\n",
    "        bl = scores_a[scores_a['student_type'] == 'unsteered']['normalized_score'].mean()\n",
    "        row[f'student_{d}_effect'] = round(mean - bl, 3)\n",
    "    \n",
    "    # Judge metrics\n",
    "    for d in ['pos', 'neg']:\n",
    "        jt = f'{trait}_{d}'\n",
    "        qwk_row = qwk_df[qwk_df['judge_type'] == jt]\n",
    "        if len(qwk_row) > 0:\n",
    "            row[f'judge_{d}_qwk'] = qwk_row['mean_qwk'].values[0]\n",
    "            row[f'judge_{d}_bias'] = qwk_row['mean_bias'].values[0]\n",
    "    \n",
    "    # Answer length effect\n",
    "    pos_len = answers_a[answers_a['student_type'] == f'{trait}_pos']['answer_len'].mean()\n",
    "    unst_len = answers_a[answers_a['student_type'] == 'unsteered']['answer_len'].mean()\n",
    "    row['len_change_pct'] = round((pos_len - unst_len) / unst_len * 100, 1)\n",
    "    \n",
    "    summary.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print('=== COMPREHENSIVE TRAIT SUMMARY ===')\n",
    "print(summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
