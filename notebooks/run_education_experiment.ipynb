{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona Vectors Education Scoring Experiment\n",
    "\n",
    "This notebook runs the education scoring experiment with steered and unsteered LLMs.\n",
    "\n",
    "**Setup:** Runtime → Change runtime type → **T4 GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and install dependencies\n",
    "!git clone https://github.com/dxnlp/persona_vectors.git\n",
    "%cd persona_vectors\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB)\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up SSH for file transfer (optional - for backing up results)\n",
    "!pip install colab_ssh -q\n",
    "from colab_ssh import launch_ssh_cloudflared\n",
    "launch_ssh_cloudflared(password=\"mypassword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Pre-computed Assets\n",
    "\n",
    "Upload the steering vectors from local machine via SSH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for vectors\n",
    "!mkdir -p persona_vectors/Qwen3-4B\n",
    "\n",
    "# Upload vectors via SSH from local terminal:\n",
    "# sshpass -p 'mypassword' scp /Users/yongchao/persona_vectors/persona_vectors/Qwen3-4B/*.pt root@<cloudflared-url>:/content/persona_vectors/persona_vectors/Qwen3-4B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify vectors are uploaded\n",
    "!ls -la persona_vectors/Qwen3-4B/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify ASAP-SAS Dataset\n",
    "\n",
    "The dataset should be in `asap-sas/train.tsv` (tab-separated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset exists\n",
    "!ls -la asap-sas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"asap-sas/train.tsv\", sep=\"\\t\")\n",
    "print(f\"Total essays: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Essay sets: {sorted(df['EssaySet'].unique())}\")\n",
    "print(f\"\\nScore distribution by set:\")\n",
    "print(df.groupby('EssaySet')['Score1'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample essay\n",
    "sample = df[df['EssaySet'] == 1].iloc[0]\n",
    "print(f\"Essay ID: {sample['Id']}\")\n",
    "print(f\"Score1: {sample['Score1']}, Score2: {sample['Score2']}\")\n",
    "print(f\"Text: {sample['EssayText'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Quick Test (1 Essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 1 essay\n",
    "!python -m experiments.education.run_experiment \\\n",
    "    --test \\\n",
    "    --essays 1 \\\n",
    "    --model Qwen/Qwen3-4B \\\n",
    "    --data-path asap-sas/train.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Full Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full experiment with 10 essays per set\n",
    "!python -m experiments.education.run_experiment \\\n",
    "    --model Qwen/Qwen3-4B \\\n",
    "    --essays 10 \\\n",
    "    --essay-sets 1 2 \\\n",
    "    --data-path asap-sas/train.tsv \\\n",
    "    --layer 15 \\\n",
    "    --coef 2.0 \\\n",
    "    --output-dir experiments/education/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manual Step-by-Step Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from eval.model_utils import load_model\n",
    "from experiments.education.config import ExperimentConfig, SteeringConfig, ESSAY_SET_INFO\n",
    "from experiments.education.data_loader import ASAPDataLoader, Essay\n",
    "from experiments.education.student import StudentGenerator, GeneratedAnswer\n",
    "from experiments.education.judge import LocalJudge, ScoringResult\n",
    "from experiments.education.metrics import calculate_qwk, calculate_agreement_stats\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B\"\n",
    "VECTOR_PATH = \"persona_vectors/Qwen3-4B\"\n",
    "DATA_PATH = \"asap-sas/train.tsv\"\n",
    "STEERING_LAYER = 15\n",
    "STEERING_COEF = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model, tokenizer = load_model(MODEL_NAME)\n",
    "print(f\"Model loaded on {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load steering vectors\n",
    "vector_file = f\"{VECTOR_PATH}/evil_response_avg_diff.pt\"\n",
    "vectors = torch.load(vector_file, weights_only=False)\n",
    "print(f\"Loaded vectors shape: {vectors.shape}\")\n",
    "print(f\"Using layer {STEERING_LAYER} for steering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load essays\n",
    "loader = ASAPDataLoader(DATA_PATH)\n",
    "essays = loader.get_essays(essay_sets=[1, 2], sample_size=5)\n",
    "print(f\"Loaded {len(essays)} essays\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample essay:\")\n",
    "print(f\"  ID: {essays[0].essay_id}\")\n",
    "print(f\"  Set: {essays[0].essay_set}\")\n",
    "print(f\"  Score: {essays[0].score1} / {essays[0].score2}\")\n",
    "print(f\"  Text: {essays[0].essay_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create steering configurations\n",
    "vector_path = f\"{VECTOR_PATH}/evil_response_avg_diff.pt\"\n",
    "\n",
    "steering_configs = [\n",
    "    SteeringConfig.good(STEERING_LAYER, STEERING_COEF, vector_path),  # Negative coef = helpful\n",
    "    SteeringConfig.evil(STEERING_LAYER, STEERING_COEF, vector_path),  # Positive coef = evil\n",
    "    SteeringConfig.unsteered(),  # No steering\n",
    "]\n",
    "\n",
    "for config in steering_configs:\n",
    "    print(f\"{config.name}: coef={config.coef}, layer={config.layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers with different steering configurations\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_answers = {}\n",
    "\n",
    "for steering in steering_configs:\n",
    "    print(f\"\\n--- Generating with {steering.name} steering ---\")\n",
    "    \n",
    "    generator = StudentGenerator(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        steering_config=steering,\n",
    "        max_tokens=300,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    answers = []\n",
    "    for essay in tqdm(essays, desc=f\"Generating ({steering.name})\"):\n",
    "        answer = generator.generate_answer(essay)\n",
    "        answers.append(answer)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    all_answers[steering.name] = answers\n",
    "    print(f\"Generated {len(answers)} answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample answers from each configuration\n",
    "for config_name, answers in all_answers.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sample answer from {config_name} student:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    if answers:\n",
    "        sample = answers[0]\n",
    "        print(f\"Prompt: {sample.prompt[:100]}...\")\n",
    "        print(f\"Answer: {sample.generated_answer[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score all answers with different judges\n",
    "all_results = []\n",
    "\n",
    "for judge_steering in steering_configs:\n",
    "    print(f\"\\n--- Scoring with {judge_steering.name} judge ---\")\n",
    "    \n",
    "    judge = LocalJudge(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        steering_config=judge_steering,\n",
    "        generate_feedback=True,\n",
    "    )\n",
    "    \n",
    "    for student_config, answers in all_answers.items():\n",
    "        print(f\"  Scoring {student_config} student answers...\")\n",
    "        for answer in tqdm(answers, desc=f\"{student_config}→{judge_steering.name}\"):\n",
    "            result = judge.score(answer)\n",
    "            all_results.append(result)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nTotal scoring results: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_data = []\n",
    "for r in all_results:\n",
    "    results_data.append({\n",
    "        \"essay_id\": r.essay_id,\n",
    "        \"essay_set\": r.essay_set,\n",
    "        \"student_config\": r.student_config,\n",
    "        \"judge_config\": r.judge_config,\n",
    "        \"predicted_score\": r.predicted_score,\n",
    "        \"ground_truth_score\": r.ground_truth_score,\n",
    "        \"feedback\": r.quality_feedback,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table: Student (rows) x Judge (columns)\n",
    "pivot = results_df.pivot_table(\n",
    "    values=\"predicted_score\",\n",
    "    index=\"student_config\",\n",
    "    columns=\"judge_config\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "print(\"Average Scores by Student-Judge Configuration:\")\n",
    "print(pivot.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bias: How much higher/lower does each judge score each student type?\n",
    "bias_matrix = pivot - pivot.mean(axis=0)\n",
    "print(\"\\nJudge Bias (difference from mean):\")\n",
    "print(bias_matrix.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Judges on Original Essays (QWK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score original essays to calculate QWK against human ground truth\n",
    "qwk_results = {}\n",
    "\n",
    "for steering in steering_configs:\n",
    "    print(f\"\\n--- Evaluating {steering.name} judge on original essays ---\")\n",
    "    \n",
    "    judge = LocalJudge(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        steering_config=steering,\n",
    "        generate_feedback=False,\n",
    "    )\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    for essay in tqdm(essays, desc=f\"Scoring ({steering.name})\"):\n",
    "        result = judge.score_essay(essay, generate_feedback=False)\n",
    "        predictions.append(result.predicted_score)\n",
    "        ground_truth.append(result.ground_truth_score)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Get score range for QWK calculation\n",
    "    min_score, max_score = ESSAY_SET_INFO.get(essays[0].essay_set, {}).get(\"score_range\", (0, 3))\n",
    "    \n",
    "    qwk = calculate_qwk(predictions, ground_truth, min_score, max_score)\n",
    "    stats = calculate_agreement_stats(predictions, ground_truth, min_score, max_score)\n",
    "    \n",
    "    qwk_results[steering.name] = {\n",
    "        \"qwk\": qwk,\n",
    "        \"mae\": stats.mean_absolute_error,\n",
    "        \"exact_match\": stats.exact_match_rate,\n",
    "    }\n",
    "    \n",
    "    print(f\"  QWK: {qwk:.4f}\")\n",
    "    print(f\"  MAE: {stats.mean_absolute_error:.4f}\")\n",
    "    print(f\"  Exact Match: {stats.exact_match_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"JUDGE EVALUATION SUMMARY (vs Human Ground Truth)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Judge':<15} {'QWK':>10} {'MAE':>10} {'Exact%':>10}\")\n",
    "print(\"-\"*60)\n",
    "for judge_name, metrics in qwk_results.items():\n",
    "    print(f\"{judge_name:<15} {metrics['qwk']:>10.4f} {metrics['mae']:>10.4f} {metrics['exact_match']*100:>9.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"experiments/education/results/{timestamp}\"\n",
    "!mkdir -p {output_dir}\n",
    "\n",
    "# Save scoring results\n",
    "results_df.to_csv(f\"{output_dir}/scoring_results.csv\", index=False)\n",
    "print(f\"Saved scoring results to {output_dir}/scoring_results.csv\")\n",
    "\n",
    "# Save QWK results\n",
    "with open(f\"{output_dir}/qwk_results.json\", \"w\") as f:\n",
    "    json.dump(qwk_results, f, indent=2)\n",
    "print(f\"Saved QWK results to {output_dir}/qwk_results.json\")\n",
    "\n",
    "# Save generated answers\n",
    "for config_name, answers in all_answers.items():\n",
    "    answers_data = [{\n",
    "        \"essay_id\": a.essay_id,\n",
    "        \"prompt\": a.prompt,\n",
    "        \"answer\": a.generated_answer,\n",
    "        \"steering_config\": a.steering_config,\n",
    "    } for a in answers]\n",
    "    with open(f\"{output_dir}/answers_{config_name}.jsonl\", \"w\") as f:\n",
    "        for item in answers_data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "print(f\"Saved generated answers to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "!zip -r results.zip experiments/education/results/\n",
    "\n",
    "from google.colab import files\n",
    "files.download('results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Heatmap of average scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=pivot.values.mean())\n",
    "plt.title(\"Average Scores: Student Type vs Judge Type\")\n",
    "plt.xlabel(\"Judge Configuration\")\n",
    "plt.ylabel(\"Student Configuration\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/heatmap_scores.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of QWK scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "judges = list(qwk_results.keys())\n",
    "qwks = [qwk_results[j][\"qwk\"] for j in judges]\n",
    "\n",
    "colors = [\"green\" if q >= 0.7 else \"orange\" if q >= 0.5 else \"red\" for q in qwks]\n",
    "plt.bar(judges, qwks, color=colors)\n",
    "plt.axhline(y=0.7, color=\"gray\", linestyle=\"--\", label=\"Acceptable threshold (0.7)\")\n",
    "plt.xlabel(\"Judge Configuration\")\n",
    "plt.ylabel(\"Quadratic Weighted Kappa (QWK)\")\n",
    "plt.title(\"Judge Quality vs Human Ground Truth\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/qwk_comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
